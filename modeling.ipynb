{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fc5daa6-042f-4410-bb2a-db7e3b721a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5debf170-d167-453d-88ee-f35e223d4a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sahm</th>\n",
       "      <th>indpro</th>\n",
       "      <th>sp500</th>\n",
       "      <th>tr10</th>\n",
       "      <th>t10yff</th>\n",
       "      <th>unrate</th>\n",
       "      <th>pcepi</th>\n",
       "      <th>payems</th>\n",
       "      <th>fedfunds</th>\n",
       "      <th>recession</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1962-02-01</th>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.016229</td>\n",
       "      <td>0.016139</td>\n",
       "      <td>-0.043737</td>\n",
       "      <td>1.650556</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.827913</td>\n",
       "      <td>0.0237</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-03-01</th>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.005350</td>\n",
       "      <td>-0.005878</td>\n",
       "      <td>-0.108990</td>\n",
       "      <td>1.078182</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.819544</td>\n",
       "      <td>0.0285</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-04-01</th>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.002130</td>\n",
       "      <td>-0.063973</td>\n",
       "      <td>-0.087455</td>\n",
       "      <td>1.043000</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.829109</td>\n",
       "      <td>0.0278</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-05-01</th>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.001066</td>\n",
       "      <td>-0.089914</td>\n",
       "      <td>0.030636</td>\n",
       "      <td>1.441818</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.817113</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-06-01</th>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.002132</td>\n",
       "      <td>-0.085381</td>\n",
       "      <td>0.035411</td>\n",
       "      <td>1.206667</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.816714</td>\n",
       "      <td>0.0268</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-01</th>\n",
       "      <td>0.37</td>\n",
       "      <td>-0.000690</td>\n",
       "      <td>-0.042506</td>\n",
       "      <td>0.330591</td>\n",
       "      <td>-0.790909</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.820381</td>\n",
       "      <td>0.0533</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-01</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.007466</td>\n",
       "      <td>0.046904</td>\n",
       "      <td>-0.056818</td>\n",
       "      <td>-0.847727</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.824685</td>\n",
       "      <td>0.0533</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-06-01</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>0.034082</td>\n",
       "      <td>-0.177010</td>\n",
       "      <td>-1.024737</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.820780</td>\n",
       "      <td>0.0533</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-01</th>\n",
       "      <td>0.53</td>\n",
       "      <td>-0.009475</td>\n",
       "      <td>0.011258</td>\n",
       "      <td>-0.056627</td>\n",
       "      <td>-1.081364</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.819624</td>\n",
       "      <td>0.0533</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-01</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.008110</td>\n",
       "      <td>0.012535</td>\n",
       "      <td>-0.379636</td>\n",
       "      <td>-1.461000</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.821736</td>\n",
       "      <td>0.0533</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>751 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            sahm    indpro     sp500      tr10    t10yff  unrate  pcepi  \\\n",
       "date                                                                      \n",
       "1962-02-01 -0.17  0.016229  0.016139 -0.043737  1.650556   0.055  0.042   \n",
       "1962-03-01 -0.17  0.005350 -0.005878 -0.108990  1.078182   0.056  0.021   \n",
       "1962-04-01 -0.10  0.002130 -0.063973 -0.087455  1.043000   0.056  0.018   \n",
       "1962-05-01 -0.07 -0.001066 -0.089914  0.030636  1.441818   0.055  0.010   \n",
       "1962-06-01  0.00 -0.002132 -0.085381  0.035411  1.206667   0.055  0.010   \n",
       "...          ...       ...       ...       ...       ...     ...    ...   \n",
       "2024-04-01  0.37 -0.000690 -0.042506  0.330591 -0.790909   0.039  0.322   \n",
       "2024-05-01  0.37  0.007466  0.046904 -0.056818 -0.847727   0.040 -0.010   \n",
       "2024-06-01  0.43  0.000622  0.034082 -0.177010 -1.024737   0.041  0.145   \n",
       "2024-07-01  0.53 -0.009475  0.011258 -0.056627 -1.081364   0.043  0.189   \n",
       "2024-08-01  0.57  0.008110  0.012535 -0.379636 -1.461000   0.042  0.112   \n",
       "\n",
       "              payems  fedfunds  recession  \n",
       "date                                       \n",
       "1962-02-01  0.827913    0.0237          0  \n",
       "1962-03-01  0.819544    0.0285          0  \n",
       "1962-04-01  0.829109    0.0278          0  \n",
       "1962-05-01  0.817113    0.0236          0  \n",
       "1962-06-01  0.816714    0.0268          0  \n",
       "...              ...       ...        ...  \n",
       "2024-04-01  0.820381    0.0533          0  \n",
       "2024-05-01  0.824685    0.0533          0  \n",
       "2024-06-01  0.820780    0.0533          0  \n",
       "2024-07-01  0.819624    0.0533          0  \n",
       "2024-08-01  0.821736    0.0533          0  \n",
       "\n",
       "[751 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collected_data = pd.read_csv('data/collected_data.csv') # Read the CSV file\n",
    "\n",
    "keep_col = ['sahm', 'indpro', 'sp500', 'tr10', 't10yff', 'unrate', 'pcepi', 'payems', 'fedfunds', 'date', 'recession']\n",
    "data = collected_data[keep_col]\n",
    "del collected_data\n",
    "# Convert 'date_column' to an index.\n",
    "data.set_index('date', inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c225dd5-4466-49b2-9226-731c554b5523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get X and y \n",
    "X = data.drop(['recession'], axis=1)\n",
    "y = data['recession']\n",
    "\n",
    "# We define the training period.\n",
    "X_train, y_train = X.loc[\"1962-02-01\":\"2012-12-01\"], y.loc[\"1962-02-01\":\"2012-12-01\"]\n",
    "# We define the test period.\n",
    "# X_test, y_test = X.loc[\"2013-01-01\":], y.loc[\"2013-01-01\":]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5dd7fc6d-55e9-4348-98b7-02444fa4cc5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    611.000000\n",
       "mean       0.135843\n",
       "std        0.342902\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        0.000000\n",
       "75%        0.000000\n",
       "max        1.000000\n",
       "Name: recession, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9206ccd1-0f02-4e75-853d-46b4b33ce237",
   "metadata": {},
   "source": [
    "### 1. Metrics\n",
    "\n",
    "Here are the main metrics we can use to get the **final score** for each **model**:\n",
    "\n",
    "1. **Precision**: It measures the proportion of true positive predictions relative to all positive predictions. It is **important** when you **want to minimize false positives**.<br>Precision = TP/(TP+FP)\n",
    "   \n",
    "2. **Recall** (Sensitivity): It measures the proportion of true positive predictions to all actual positive cases. It is useful when you want to minimize false negatives.<br>Recall = TP/(TP+FN)\n",
    "   \n",
    "3. **F1-score**: Is a metric that balances precision and recall. It is calculated as the harmonic mean of precision and recall. F1 Score is useful when seeking a balance between high precision and high recall, as it penalizes extreme negative values of either component.<br>F1 = 2\\*Precision\\*Recall/(Precision+Recall)\n",
    "   \n",
    "4. **Confusion Matrix**: Visualizes true and predicted classes, which can help better understand model performance.\n",
    "    \n",
    "5. **Specificity**: It measures the proportion of true negative predictions relative to all actual negative cases. It is useful when you want to **minimize false positives**.<br>Specificity = TN/(TN+FP)\n",
    "    \n",
    "6. **Accuracy**: It measures the proportion of correctly predicted cases (both positive and negative) relative to the total number of cases. Accuracy **can be misleading with unbalanced data**, as it can be high even if the model does not predict the small class well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f73fb8-1297-4596-ab4d-0ff831d03dce",
   "metadata": {},
   "source": [
    "### 2. Logistic Regression\n",
    "\n",
    "When properly configured with the **class_weight='balanced'** option, Logistic Regression can handle unbalanced data. However, if the classes are very imbalanced, the model may be biased towards the majority class. It works well for moderate imbalance, but **may need additional techniques to deal with a large imbalance**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b126d0a9-2302-46ce-941e-c745610dad56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in y_train: [528  83]\n",
      "Precision: 0.4328\n",
      "Recall: 0.6988\n",
      "F1-Score: 0.5346\n",
      "Accuracy: 0.8347\n",
      "Confusion Matrix: \n",
      "[[452  76]\n",
      " [ 25  58]]\n",
      "Specificity: 0.8561\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "# Checking the distribution of classes in the target variable (y_train).\n",
    "print(\"Class distribution in y_train:\", np.bincount(y_train))\n",
    "# Check for minimum amount of class 1 examples.\n",
    "if np.bincount(y_train).shape[0] < 2:\n",
    "    print(\"One of the classes in the training data is missing.\")\n",
    "else:\n",
    "    # Stratified cross-validation with 5 folds.\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize logistic regression with class_weight='balanced'.\n",
    "    model = LogisticRegression(class_weight='balanced', random_state=42)\n",
    "\n",
    "    # We use cross_val_predict to predict the results during cross-validation.\n",
    "    y_pred = cross_val_predict(model, X_train, y_train, cv=skf)\n",
    "\n",
    "    # Calculation of model evaluation metrics.\n",
    "    precision = precision_score(y_train, y_pred, average='binary')\n",
    "    recall = recall_score(y_train, y_pred, average='binary')\n",
    "    f1 = f1_score(y_train, y_pred, average='binary')\n",
    "    accuracy = accuracy_score(y_train, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_train, y_pred)\n",
    "\n",
    "    # Extracting the values â€‹â€‹from the confusion matrix.\n",
    "    TN, FP, FN, TP = conf_matrix.ravel()\n",
    "\n",
    "    # Specificity: Calculates how well the model recognizes negative examples (specificity).\n",
    "    specificity = TN / (TN + FP)\n",
    "\n",
    "    # Print the results.\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Confusion Matrix: \\n{conf_matrix}\")\n",
    "    print(f\"Specificity: {specificity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06602bc2-9bf3-4cc2-b0b1-34a2a50b1a39",
   "metadata": {},
   "source": [
    "### 3. Decision Tree\n",
    "\n",
    "Decision Trees can handle unbalanced data, but without regularization they tend to be biased towards the majority class if the classes are highly imbalanced. **Not the best choice for a large imbalance if no balancing techniques are used.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d47abf4-cd08-4625-8561-69da1730481f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04d00313-33e2-4222-96eb-969fa3929767",
   "metadata": {},
   "source": [
    "### 4. Random Forest\n",
    "\n",
    "Random Forest is more robust to unbalanced classes, especially when used with the **class_weight='balanced_subsample'** option, which compensates for the imbalance. This makes it more stable compared to Decision Tree. **Very suitable for unbalanced data if properly set up.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898cd308-b094-4a91-8fc5-09959ef2b37d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1b076d9-4b3d-4fb4-b029-c1d3570b4003",
   "metadata": {},
   "source": [
    "### 5. XGBoost\n",
    "\n",
    "XGBoost has the **scale_pos_weight** parameter which can correct the imbalance between classes. This makes it one of the best models for unbalanced data. One of the most suitable models **to deal with severe imbalance**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d2214a-2d2a-46d5-b1b5-d766ba8a7ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4a76744-746a-4348-a78a-b40fb6b32858",
   "metadata": {},
   "source": [
    "### 6. CatBoost\n",
    "\n",
    "CatBoost also supports imbalance correction parameters (**class_weights**) and can automatically detect imbalance in data. Good **for dealing with unbalanced classes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7355d09e-d8e5-4770-8b33-23f1a77144eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6940e204-7a95-42d5-a011-bc721f2faf9a",
   "metadata": {},
   "source": [
    "### 7. SVM\n",
    "\n",
    "SVM can handle unbalanced classes by using the **class_weight='balanced'** parameter. However, with highly unbalanced data, SVM may not be the best solution. It works well with moderate imbalance, but **may struggle with more imbalance**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fcad63-49cb-4ff6-93b9-f3b0ad06c888",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
