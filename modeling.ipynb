{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fc5daa6-042f-4410-bb2a-db7e3b721a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5debf170-d167-453d-88ee-f35e223d4a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sahm</th>\n",
       "      <th>indpro</th>\n",
       "      <th>sp500</th>\n",
       "      <th>tr10</th>\n",
       "      <th>t10yff</th>\n",
       "      <th>unrate</th>\n",
       "      <th>pcepi</th>\n",
       "      <th>payems</th>\n",
       "      <th>recession</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.016229</td>\n",
       "      <td>0.016139</td>\n",
       "      <td>-0.043737</td>\n",
       "      <td>1.650556</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.827913</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.005350</td>\n",
       "      <td>-0.005878</td>\n",
       "      <td>-0.108990</td>\n",
       "      <td>1.078182</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.819544</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.002130</td>\n",
       "      <td>-0.063973</td>\n",
       "      <td>-0.087455</td>\n",
       "      <td>1.043000</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.829109</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.001066</td>\n",
       "      <td>-0.089914</td>\n",
       "      <td>0.030636</td>\n",
       "      <td>1.441818</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.817113</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.002132</td>\n",
       "      <td>-0.085381</td>\n",
       "      <td>0.035411</td>\n",
       "      <td>1.206667</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.816714</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>0.37</td>\n",
       "      <td>-0.000690</td>\n",
       "      <td>-0.042506</td>\n",
       "      <td>0.330591</td>\n",
       "      <td>-0.790909</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.820381</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.007466</td>\n",
       "      <td>0.046904</td>\n",
       "      <td>-0.056818</td>\n",
       "      <td>-0.847727</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.824685</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>0.034082</td>\n",
       "      <td>-0.177010</td>\n",
       "      <td>-1.024737</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.820780</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>0.53</td>\n",
       "      <td>-0.009475</td>\n",
       "      <td>0.011258</td>\n",
       "      <td>-0.056627</td>\n",
       "      <td>-1.081364</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.819624</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.008110</td>\n",
       "      <td>0.012535</td>\n",
       "      <td>-0.379636</td>\n",
       "      <td>-1.461000</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.821736</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>751 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sahm    indpro     sp500      tr10    t10yff  unrate  pcepi    payems  \\\n",
       "0   -0.17  0.016229  0.016139 -0.043737  1.650556   0.055  0.042  0.827913   \n",
       "1   -0.17  0.005350 -0.005878 -0.108990  1.078182   0.056  0.021  0.819544   \n",
       "2   -0.10  0.002130 -0.063973 -0.087455  1.043000   0.056  0.018  0.829109   \n",
       "3   -0.07 -0.001066 -0.089914  0.030636  1.441818   0.055  0.010  0.817113   \n",
       "4    0.00 -0.002132 -0.085381  0.035411  1.206667   0.055  0.010  0.816714   \n",
       "..    ...       ...       ...       ...       ...     ...    ...       ...   \n",
       "746  0.37 -0.000690 -0.042506  0.330591 -0.790909   0.039  0.322  0.820381   \n",
       "747  0.37  0.007466  0.046904 -0.056818 -0.847727   0.040 -0.010  0.824685   \n",
       "748  0.43  0.000622  0.034082 -0.177010 -1.024737   0.041  0.145  0.820780   \n",
       "749  0.53 -0.009475  0.011258 -0.056627 -1.081364   0.043  0.189  0.819624   \n",
       "750  0.57  0.008110  0.012535 -0.379636 -1.461000   0.042  0.112  0.821736   \n",
       "\n",
       "     recession  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  \n",
       "..         ...  \n",
       "746          0  \n",
       "747          0  \n",
       "748          0  \n",
       "749          0  \n",
       "750          0  \n",
       "\n",
       "[751 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collected_data = pd.read_csv('data/collected_data.csv') # Read the CSV file\n",
    "\n",
    "keep_col = ['sahm', 'indpro', 'sp500', 'tr10', 't10yff', 'unrate', 'pcepi', 'payems', 'recession']\n",
    "data = collected_data[keep_col]\n",
    "del collected_data\n",
    "# data['date'] = pd.to_datetime(data['date'])\n",
    "# data['date'] = (data['date'] - data['date'].min()).dt.days  # Convert to days.\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3859275f-9372-4bf4-b6ef-5a90d8ff237a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 751 entries, 0 to 750\n",
      "Data columns (total 9 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   sahm       751 non-null    float64\n",
      " 1   indpro     751 non-null    float64\n",
      " 2   sp500      751 non-null    float64\n",
      " 3   tr10       751 non-null    float64\n",
      " 4   t10yff     751 non-null    float64\n",
      " 5   unrate     751 non-null    float64\n",
      " 6   pcepi      751 non-null    float64\n",
      " 7   payems     751 non-null    float64\n",
      " 8   recession  751 non-null    int64  \n",
      "dtypes: float64(8), int64(1)\n",
      "memory usage: 52.9 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c225dd5-4466-49b2-9226-731c554b5523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get X and y \n",
    "X = data.drop(['recession'], axis=1)\n",
    "y = data[\"recession\"]\n",
    "\n",
    "# We define the training period.\n",
    "X_train, y_train = X.loc[\"1962-02-01\":\"2012-12-01\"], y.loc[\"1962-02-01\":\"2012-12-01\"]\n",
    "# We define the test period.\n",
    "X_test, y_test = X.loc[\"2013-01-01\":], y.loc[\"2013-01-01\":]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9206ccd1-0f02-4e75-853d-46b4b33ce237",
   "metadata": {},
   "source": [
    "### 1. Metrics\n",
    "\n",
    "Here are the main metrics we can use to get the **final score** for each **model**:\n",
    "\n",
    "1. **Precision**: It measures the proportion of true positive predictions relative to all positive predictions. It is **important** when you **want to minimize false positives**.<br>Precision = TP/(TP+FP)\n",
    "   \n",
    "2. **Recall** (Sensitivity): It measures the proportion of true positive predictions to all actual positive cases. It is useful when you want to minimize false negatives.<br>Recall = TP/(TP+FN)\n",
    "   \n",
    "3. **F1-score**: Is a metric that balances precision and recall. It is calculated as the harmonic mean of precision and recall. F1 Score is useful when seeking a balance between high precision and high recall, as it penalizes extreme negative values of either component.<br>F1 = 2\\*Precision\\*Recall/(Precision+Recall)\n",
    "   \n",
    "4. **Confusion Matrix**: Visualizes true and predicted classes, which can help better understand model performance.\n",
    "    \n",
    "5. **Specificity**: It measures the proportion of true negative predictions relative to all actual negative cases. It is useful when you want to **minimize false positives**.<br>Specificity = TN/(TN+FP)\n",
    "    \n",
    "6. **Accuracy**: It measures the proportion of correctly predicted cases (both positive and negative) relative to the total number of cases. Accuracy **can be misleading with unbalanced data**, as it can be high even if the model does not predict the small class well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f73fb8-1297-4596-ab4d-0ff831d03dce",
   "metadata": {},
   "source": [
    "### 2. Logistic Regression\n",
    "\n",
    "When properly configured with the **class_weight='balanced'** option, Logistic Regression can handle unbalanced data. However, if the classes are very imbalanced, the model may be biased towards the majority class. It works well for moderate imbalance, but **may need additional techniques to deal with a large imbalance**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a475e0-7728-45f0-9f7c-2f3c1f8657f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06602bc2-9bf3-4cc2-b0b1-34a2a50b1a39",
   "metadata": {},
   "source": [
    "### 3. Decision Tree\n",
    "\n",
    "Decision Trees can handle unbalanced data, but without regularization they tend to be biased towards the majority class if the classes are highly imbalanced. **Not the best choice for a large imbalance if no balancing techniques are used.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d47abf4-cd08-4625-8561-69da1730481f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04d00313-33e2-4222-96eb-969fa3929767",
   "metadata": {},
   "source": [
    "### 4. Random Forest\n",
    "\n",
    "Random Forest is more robust to unbalanced classes, especially when used with the **class_weight='balanced_subsample'** option, which compensates for the imbalance. This makes it more stable compared to Decision Tree. **Very suitable for unbalanced data if properly set up.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898cd308-b094-4a91-8fc5-09959ef2b37d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1b076d9-4b3d-4fb4-b029-c1d3570b4003",
   "metadata": {},
   "source": [
    "### 5. XGBoost\n",
    "\n",
    "XGBoost has the **scale_pos_weight** parameter which can correct the imbalance between classes. This makes it one of the best models for unbalanced data. One of the most suitable models **to deal with severe imbalance**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d2214a-2d2a-46d5-b1b5-d766ba8a7ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4a76744-746a-4348-a78a-b40fb6b32858",
   "metadata": {},
   "source": [
    "### 6. CatBoost\n",
    "\n",
    "CatBoost also supports imbalance correction parameters (**class_weights**) and can automatically detect imbalance in data. Good **for dealing with unbalanced classes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7355d09e-d8e5-4770-8b33-23f1a77144eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6940e204-7a95-42d5-a011-bc721f2faf9a",
   "metadata": {},
   "source": [
    "### 7. SVM\n",
    "\n",
    "SVM can handle unbalanced classes by using the **class_weight='balanced'** parameter. However, with highly unbalanced data, SVM may not be the best solution. It works well with moderate imbalance, but **may struggle with more imbalance**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fcad63-49cb-4ff6-93b9-f3b0ad06c888",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
